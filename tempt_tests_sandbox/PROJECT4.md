3. データパイプライン設計

本プロジェクトでは、データの「収集」から「予測」までを4つのフェーズに分離したパイプラインを構築する。これにより、一度集めたデータを無駄にせず、モデルの改善を高速化する。
Phase 1: Ingestion（データ収集）

    役割: pyjpboatrace を使い、公式サイトのHTMLを解析して生データとして保存する。

    出力: races.csv, entries.csv, results.csv の3点。

    メリット: 公式サイトへの負荷を最小限に抑え、手元に「自分だけのデータベース」を構築できる。

Phase 2: Transformation（データ結合・変換）

    役割: 3つのCSVを race_id で結合（JOIN）し、機械学習アルゴリズムが理解できる「1行1艇」のフラットな表形式に変換する。

    出力: training_base.csv（学習用ベースデータ）

    処理内容:

        entries（選手情報）に results（着順）を紐づける。

        欠損値（欠場など）の除外。

Phase 3: Feature Engineering（特徴量生成）

    役割: 素のデータから「勝てるヒント」を計算する。

    具体例:

        平均STと展示STの差を算出。

        「この選手は2コースの時にどれだけ2連対しているか」等の集計指標の追加。

    メリット: ここを工夫するだけで、アルゴリズムを変えずとも予測精度が劇的に向上する。

Phase 4: Training & Evaluation（学習・評価）

    役割: LightGBM等のモデルにデータを投入し、学習を行う。

    出力: model.pkl（学習済みモデルファイル）

    検証: 過去データで「2連対」をどれだけ当てられたか、公式予想と比較して評価する。

4. データの保存形式（詳細）
4.1 共通キーの定義

全てのファイルを紐づけるため、以下の形式で race_id を生成し、全CSVに付与する。

    race_id = YYYYMMDD (日付) + stadium_id (場コード) + race_no (レース番号)
    例: 20240101_01_12 (2024年1月1日 桐生 12R)

4.2 保存先ディレクトリとスキーマ
ファイル名	粒度	必須カラム（抜粋）
races.csv	1レース/1行	race_id, date, stadium_id, title, deadline
entries.csv	1艇/1行	race_id, boat_no, racer_id, name, class, motor_p, st_ave
results.csv	1レース/1行	race_id, rank1_boat, rank2_boat, rank3_boat, payoff_3t
5. 運用ルール・制約

    オッズデータの非採用: 開発初期段階ではオッズ（odds.csv）は扱わない。

        理由1: サイトへのアクセス過多によるアクセス禁止リスクを避けるため。

        理由2: モデルが「人気の艇＝勝つ」という安易な答えに依存するのを防ぐため。

    実行環境の厳守: スクレイピング実行時は、相手サーバーへの礼儀として必ずリクエスト間に time.sleep(1) 以上の待機時間を設ける。